{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 784)\n",
      "(1500, 28, 28)\n",
      "(1000, 28, 28) (500, 28, 28)\n",
      "(1000, 1, 28, 28) (500, 1, 28, 28)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"自作の画像1500枚を読み込みます\"\"\"\n",
    "import cv2\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "#画像の読み込み訓練画像1000枚、テスト画像500枚\n",
    "\n",
    "image_dir = ['./train_img', './test_img']\n",
    "search_pattern = '*'\n",
    "\n",
    "datas = []\n",
    "\n",
    "for i in range(len(image_dir)):\n",
    "    for image_path in sorted(glob.glob(os.path.join(image_dir[i],search_pattern))):\n",
    "        data = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  #３チャンネルを１に変更\n",
    "        data_expanded = np.expand_dims(data,axis=0)\n",
    "        datas.append(data_expanded)\n",
    "        \n",
    "image = np.concatenate(datas,axis=0)\n",
    "image = image.reshape(1500, 784)\n",
    "print(image.shape)\n",
    "\n",
    "img = []\n",
    "for n in range(0,1500):\n",
    "    for i in range(0, 784):\n",
    "        image[n,i] = image[n,i].astype(np.float32)\n",
    "        img.append((255.0 - image[n,i]) / 255.0)  #計算の都合上、白と黒を反転\n",
    "        \n",
    "img = np.array(img)\n",
    "img = img.reshape(1500, 28, 28)\n",
    "print(img.shape)\n",
    "\n",
    "#訓練データとテストデータに分けます\n",
    "train_img = img[:1000]\n",
    "test_img = img[1000:]\n",
    "\n",
    "#それぞれのデータの次元確認\n",
    "print(train_img.shape, test_img.shape)\n",
    "\n",
    "#データが順番に読み込んでいるか確認\n",
    "for i in range(51):\n",
    "    plt.imshow(test_img[i], cmap = 'gray')\n",
    "    \n",
    "x_train = train_img.reshape(1000, 1, 28, 28) \n",
    "x_test = test_img.reshape(500, 1, 28, 28)\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(type(train_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "(1000, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "(500, 10)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"訓練とテストのラベル（正解例）を作成\"\"\"\n",
    "tr0 = [0]*100\n",
    "tr1 = [1]*100\n",
    "tr2 = [2]*100\n",
    "tr3 = [3]*100\n",
    "tr4 = [4]*100\n",
    "tr5 = [5]*100\n",
    "tr6 = [6]*100\n",
    "tr7 = [7]*100\n",
    "tr8 = [8]*100\n",
    "tr9 = [9]*100\n",
    "t_tr= tr0 + tr1 + tr2 + tr3 + tr4 + tr5 + tr6 + tr7 + tr8 + tr9\n",
    "t_train = np.identity(10)[t_tr]\n",
    "\n",
    "print(t_train)\n",
    "print(t_train.shape)\n",
    "\n",
    "te0 = [0]*50\n",
    "te1 = [1]*50\n",
    "te2 = [2]*50\n",
    "te3 = [3]*50\n",
    "te4 = [4]*50\n",
    "te5 = [5]*50\n",
    "te6 = [6]*50\n",
    "te7 = [7]*50\n",
    "te8 = [8]*50\n",
    "te9 = [9]*50\n",
    "t_te = te0 + te1 + te2 + te3 + te4 + te5 + te6 + te7 + te8 + te9\n",
    "t_test = np.identity(10)[t_te]\n",
    "print(t_test)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.575276183876188\n",
      "=== epoch:1, train acc:0.125, test acc:0.15 ===\n",
      "train loss:2.569111049458083\n",
      "train loss:2.6504017946930936\n",
      "train loss:2.4720889729638675\n",
      "train loss:2.473107394300363\n",
      "train loss:2.449502808547664\n",
      "train loss:2.4322057987345946\n",
      "train loss:2.542418825695174\n",
      "train loss:2.6374779161273527\n",
      "train loss:2.52181258379192\n",
      "train loss:2.461977326547288\n",
      "=== epoch:2, train acc:0.142, test acc:0.172 ===\n",
      "train loss:2.429893194550034\n",
      "train loss:2.3138194808407713\n",
      "train loss:2.5632835208699576\n",
      "train loss:2.375989063087414\n",
      "train loss:2.4837135272222017\n",
      "train loss:2.319903623899199\n",
      "train loss:2.4114471735245426\n",
      "train loss:2.512175723704594\n",
      "train loss:2.3345126682055795\n",
      "train loss:2.392905007482613\n",
      "=== epoch:3, train acc:0.16, test acc:0.188 ===\n",
      "train loss:2.472893639590701\n",
      "train loss:2.5129895945970113\n",
      "train loss:2.31469000020483\n",
      "train loss:2.3155047896966474\n",
      "train loss:2.4471427429374137\n",
      "train loss:2.5380410514280523\n",
      "train loss:2.2827263613517874\n",
      "train loss:2.4498913063702874\n",
      "train loss:2.307957846821962\n",
      "train loss:2.2893584271303773\n",
      "=== epoch:4, train acc:0.186, test acc:0.186 ===\n",
      "train loss:2.332407244745737\n",
      "train loss:2.3829288153045303\n",
      "train loss:2.3577294944157305\n",
      "train loss:2.363771358326014\n",
      "train loss:2.456802762739317\n",
      "train loss:2.2744144896363467\n",
      "train loss:2.266039435576053\n",
      "train loss:2.228640083232762\n",
      "train loss:2.210400044980545\n",
      "train loss:2.4889038073302867\n",
      "=== epoch:5, train acc:0.211, test acc:0.192 ===\n",
      "train loss:2.299130103888013\n",
      "train loss:2.324310305799376\n",
      "train loss:2.258918434813823\n",
      "train loss:2.2603636648133714\n",
      "train loss:2.237674658029419\n",
      "train loss:2.306143571611137\n",
      "train loss:2.3328977809620577\n",
      "train loss:2.3837905478189843\n",
      "train loss:2.22914809006505\n",
      "train loss:2.375742277385082\n",
      "=== epoch:6, train acc:0.228, test acc:0.202 ===\n",
      "train loss:2.228384289054182\n",
      "train loss:2.4169263447863827\n",
      "train loss:2.273512448543101\n",
      "train loss:2.3179060356554086\n",
      "train loss:2.3254206852673875\n",
      "train loss:2.2387025405149283\n",
      "train loss:2.2459210937855185\n",
      "train loss:2.2195044458695805\n",
      "train loss:2.2143694758204164\n",
      "train loss:2.1299022756362853\n",
      "=== epoch:7, train acc:0.248, test acc:0.226 ===\n",
      "train loss:2.234912842232692\n",
      "train loss:2.2871841624749902\n",
      "train loss:2.41959733311061\n",
      "train loss:2.3475799730206166\n",
      "train loss:2.2277245814429523\n",
      "train loss:2.345524541884744\n",
      "train loss:2.2483700454368614\n",
      "train loss:2.1695169671447148\n",
      "train loss:2.1170100064949735\n",
      "train loss:2.140492914258788\n",
      "=== epoch:8, train acc:0.271, test acc:0.232 ===\n",
      "train loss:2.129646074530257\n",
      "train loss:2.1031163480386983\n",
      "train loss:2.1665337047773603\n",
      "train loss:2.1278207772219933\n",
      "train loss:2.190087393020615\n",
      "train loss:2.2388759617460052\n",
      "train loss:2.086136279793561\n",
      "train loss:2.2313333273684774\n",
      "train loss:2.138083577933696\n",
      "train loss:2.168391027185204\n",
      "=== epoch:9, train acc:0.294, test acc:0.236 ===\n",
      "train loss:2.062779066120254\n",
      "train loss:2.2091554350650897\n",
      "train loss:2.156982071456732\n",
      "train loss:2.026307829659794\n",
      "train loss:2.0760145528722815\n",
      "train loss:2.1855642663828885\n",
      "train loss:2.054731497826701\n",
      "train loss:2.1716723066280714\n",
      "train loss:2.1395520427637194\n",
      "train loss:2.146407170664676\n",
      "=== epoch:10, train acc:0.308, test acc:0.25 ===\n",
      "train loss:2.1606356764997283\n",
      "train loss:2.1381848755017057\n",
      "train loss:2.157204264301791\n",
      "train loss:2.1454061351717475\n",
      "train loss:2.0807165571324164\n",
      "train loss:2.0394419682603813\n",
      "train loss:2.1505457065601576\n",
      "train loss:2.200408486663579\n",
      "train loss:2.1702668801324516\n",
      "train loss:2.0650486409531412\n",
      "=== epoch:11, train acc:0.324, test acc:0.264 ===\n",
      "train loss:2.220863573577264\n",
      "train loss:2.0912106564844195\n",
      "train loss:2.069772356269633\n",
      "train loss:2.1674219438126854\n",
      "train loss:2.165601179591844\n",
      "train loss:2.0787677127784594\n",
      "train loss:2.051867075506461\n",
      "train loss:2.080884447383536\n",
      "train loss:2.116179379190558\n",
      "train loss:2.1387441654919557\n",
      "=== epoch:12, train acc:0.347, test acc:0.27 ===\n",
      "train loss:2.068149978715266\n",
      "train loss:2.1154079148748814\n",
      "train loss:2.056625522386162\n",
      "train loss:2.0557519331960368\n",
      "train loss:2.037275366829975\n",
      "train loss:2.0850408551019153\n",
      "train loss:1.9882806003402365\n",
      "train loss:1.9599856408388843\n",
      "train loss:1.9915217662428606\n",
      "train loss:2.057500918539935\n",
      "=== epoch:13, train acc:0.362, test acc:0.276 ===\n",
      "train loss:2.034239284343024\n",
      "train loss:2.0476660584946256\n",
      "train loss:2.0090523000193126\n",
      "train loss:2.0446558193040705\n",
      "train loss:2.068501958508099\n",
      "train loss:1.9237917971071368\n",
      "train loss:1.9791935194377621\n",
      "train loss:1.9712015199653061\n",
      "train loss:1.8882289863670383\n",
      "train loss:2.051189860445069\n",
      "=== epoch:14, train acc:0.373, test acc:0.282 ===\n",
      "train loss:2.050177482119869\n",
      "train loss:2.0180835003779136\n",
      "train loss:1.9727499800023616\n",
      "train loss:2.0546111090343087\n",
      "train loss:1.9245043226147425\n",
      "train loss:1.97546768905033\n",
      "train loss:1.9154693568071426\n",
      "train loss:1.9176010675082802\n",
      "train loss:2.014817443065185\n",
      "train loss:2.0219745112774645\n",
      "=== epoch:15, train acc:0.391, test acc:0.302 ===\n",
      "train loss:2.003293495007364\n",
      "train loss:1.9602829911947885\n",
      "train loss:1.9164500885277878\n",
      "train loss:1.9258327111066385\n",
      "train loss:1.9544634287571327\n",
      "train loss:1.9627634381309378\n",
      "train loss:1.9254271221173527\n",
      "train loss:2.0499488236431382\n",
      "train loss:1.9745666323773319\n",
      "train loss:1.9576575117208146\n",
      "=== epoch:16, train acc:0.408, test acc:0.316 ===\n",
      "train loss:1.9379091370226231\n",
      "train loss:2.0027391187916077\n",
      "train loss:2.0251429143264947\n",
      "train loss:1.9454374281157636\n",
      "train loss:1.9101163342608067\n",
      "train loss:2.116629351774277\n",
      "train loss:2.0026234053161125\n",
      "train loss:1.9880083375763666\n",
      "train loss:1.9710560195865932\n",
      "train loss:1.9133283858490253\n",
      "=== epoch:17, train acc:0.424, test acc:0.326 ===\n",
      "train loss:1.9544602175539216\n",
      "train loss:1.8635528165246138\n",
      "train loss:1.890146033738068\n",
      "train loss:1.8400241051074675\n",
      "train loss:1.9031585962591857\n",
      "train loss:1.9111666865633563\n",
      "train loss:1.917658974851317\n",
      "train loss:1.925067805051867\n",
      "train loss:1.876458258630156\n",
      "train loss:1.8466392887578649\n",
      "=== epoch:18, train acc:0.438, test acc:0.336 ===\n",
      "train loss:1.9623423904786792\n",
      "train loss:1.9574279187840917\n",
      "train loss:1.8853056158673518\n",
      "train loss:1.8948398250767724\n",
      "train loss:1.8412950198909304\n",
      "train loss:1.8098306015391947\n",
      "train loss:1.9920681361682404\n",
      "train loss:1.8208695210065857\n",
      "train loss:1.8199864175499136\n",
      "train loss:1.8523255523103046\n",
      "=== epoch:19, train acc:0.455, test acc:0.342 ===\n",
      "train loss:1.8621989798890548\n",
      "train loss:1.830011443044528\n",
      "train loss:2.0067709067576947\n",
      "train loss:1.8270827622366832\n",
      "train loss:1.8633404610379074\n",
      "train loss:1.8481873973060359\n",
      "train loss:1.7673780890152841\n",
      "train loss:1.9767499269180702\n",
      "train loss:1.8304083346466755\n",
      "train loss:1.8830822543023242\n",
      "=== epoch:20, train acc:0.469, test acc:0.344 ===\n",
      "train loss:1.8364453295498109\n",
      "train loss:1.8398018711600403\n",
      "train loss:1.8779062357152672\n",
      "train loss:1.835215485815266\n",
      "train loss:1.938770485123097\n",
      "train loss:1.8404441466207009\n",
      "train loss:1.7781065870132335\n",
      "train loss:1.8349486072876022\n",
      "train loss:1.808331592896566\n",
      "train loss:1.8148295173174196\n",
      "=== epoch:21, train acc:0.488, test acc:0.354 ===\n",
      "train loss:1.8188753449779298\n",
      "train loss:1.770226839872484\n",
      "train loss:1.7963887694037706\n",
      "train loss:1.8314215203725328\n",
      "train loss:1.8205256006338908\n",
      "train loss:1.854377207032103\n",
      "train loss:1.8782125378214178\n",
      "train loss:1.8126776556674908\n",
      "train loss:1.8217805075254403\n",
      "train loss:1.7860189036119996\n",
      "=== epoch:22, train acc:0.497, test acc:0.362 ===\n",
      "train loss:1.7472863732930162\n",
      "train loss:1.671866609575067\n",
      "train loss:1.7772129036685016\n",
      "train loss:1.83231652110451\n",
      "train loss:1.6971939449831641\n",
      "train loss:1.8280620420668752\n",
      "train loss:1.7562694327665298\n",
      "train loss:1.8067122630954922\n",
      "train loss:1.726524266507888\n",
      "train loss:1.7664634694779684\n",
      "=== epoch:23, train acc:0.504, test acc:0.364 ===\n",
      "train loss:1.7229298691900414\n",
      "train loss:1.7697936789607092\n",
      "train loss:1.7787810937959665\n",
      "train loss:1.785571538180529\n",
      "train loss:1.788906681425681\n",
      "train loss:1.7549198068459662\n",
      "train loss:1.8139822752526817\n",
      "train loss:1.7860045403283338\n",
      "train loss:1.6598827478829177\n",
      "train loss:1.75879381197508\n",
      "=== epoch:24, train acc:0.523, test acc:0.366 ===\n",
      "train loss:1.7115103562974718\n",
      "train loss:1.7158516423806518\n",
      "train loss:1.6928527400847913\n",
      "train loss:1.8144554681175118\n",
      "train loss:1.779257734718996\n",
      "train loss:1.7128968275694996\n",
      "train loss:1.8191041809395887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.7650466559404159\n",
      "train loss:1.6312479405772669\n",
      "train loss:1.6560859641201962\n",
      "=== epoch:25, train acc:0.54, test acc:0.38 ===\n",
      "train loss:1.767470427104109\n",
      "train loss:1.750158328878301\n",
      "train loss:1.6750350120831439\n",
      "train loss:1.7139291140100517\n",
      "train loss:1.6321099846118683\n",
      "train loss:1.6431616231433523\n",
      "train loss:1.633561111745818\n",
      "train loss:1.6313931976087335\n",
      "train loss:1.7601737230763497\n",
      "train loss:1.692920264930983\n",
      "=== epoch:26, train acc:0.552, test acc:0.384 ===\n",
      "train loss:1.7517534448463836\n",
      "train loss:1.72607064472819\n",
      "train loss:1.7021219313323916\n",
      "train loss:1.7328123241565458\n",
      "train loss:1.6663417219154673\n",
      "train loss:1.6578614256742517\n",
      "train loss:1.6965167710069602\n",
      "train loss:1.6681000768999052\n",
      "train loss:1.680858343641424\n",
      "train loss:1.6310482427640205\n",
      "=== epoch:27, train acc:0.57, test acc:0.402 ===\n",
      "train loss:1.5864936796537281\n",
      "train loss:1.5839014499019695\n",
      "train loss:1.6278682945898815\n",
      "train loss:1.569392320779552\n",
      "train loss:1.6595284401345032\n",
      "train loss:1.5986962604985147\n",
      "train loss:1.6858210926193888\n",
      "train loss:1.6405234248850147\n",
      "train loss:1.6846636304980338\n",
      "train loss:1.5386954249066735\n",
      "=== epoch:28, train acc:0.579, test acc:0.41 ===\n",
      "train loss:1.6774812823175302\n",
      "train loss:1.6142677265961227\n",
      "train loss:1.6988591278888634\n",
      "train loss:1.593795560311947\n",
      "train loss:1.5789334876180203\n",
      "train loss:1.665858769377688\n",
      "train loss:1.6943264200057597\n",
      "train loss:1.6052353620726998\n",
      "train loss:1.582608387995748\n",
      "train loss:1.6260668557326925\n",
      "=== epoch:29, train acc:0.59, test acc:0.422 ===\n",
      "train loss:1.6634096549159432\n",
      "train loss:1.5506968446175597\n",
      "train loss:1.6068370750127088\n",
      "train loss:1.650956539493277\n",
      "train loss:1.646746539932837\n",
      "train loss:1.5866033810153806\n",
      "train loss:1.6881429520237967\n",
      "train loss:1.5219947591459277\n",
      "train loss:1.6314726255275245\n",
      "train loss:1.561456123437753\n",
      "=== epoch:30, train acc:0.616, test acc:0.43 ===\n",
      "train loss:1.6669419495086115\n",
      "train loss:1.5563951770535343\n",
      "train loss:1.6107427382528448\n",
      "train loss:1.5789344930018205\n",
      "train loss:1.5309137769502434\n",
      "train loss:1.541679816314164\n",
      "train loss:1.6433436689251837\n",
      "train loss:1.5036071752297615\n",
      "train loss:1.4917333907124932\n",
      "train loss:1.6032068590501143\n",
      "=== epoch:31, train acc:0.633, test acc:0.442 ===\n",
      "train loss:1.5756919219279604\n",
      "train loss:1.5163769589288476\n",
      "train loss:1.5582102124904023\n",
      "train loss:1.5690466001842898\n",
      "train loss:1.4413106424009083\n",
      "train loss:1.5336462553033159\n",
      "train loss:1.5097906533569576\n",
      "train loss:1.5718571870273317\n",
      "train loss:1.5885736324032569\n",
      "train loss:1.564185362159646\n",
      "=== epoch:32, train acc:0.645, test acc:0.448 ===\n",
      "train loss:1.5240489407191395\n",
      "train loss:1.4701261352203392\n",
      "train loss:1.5114650895316435\n",
      "train loss:1.533161381435476\n",
      "train loss:1.4793588366377282\n",
      "train loss:1.4946062563980231\n",
      "train loss:1.4785914918757834\n",
      "train loss:1.4669159623077062\n",
      "train loss:1.5565064728608187\n",
      "train loss:1.5066081919341048\n",
      "=== epoch:33, train acc:0.659, test acc:0.464 ===\n",
      "train loss:1.4850389728390658\n",
      "train loss:1.4307373707284308\n",
      "train loss:1.519427178906279\n",
      "train loss:1.4829347883831054\n",
      "train loss:1.4253711416060642\n",
      "train loss:1.4751010395800928\n",
      "train loss:1.5004477180609337\n",
      "train loss:1.4329477729768458\n",
      "train loss:1.4716393050288394\n",
      "train loss:1.4134216473518106\n",
      "=== epoch:34, train acc:0.67, test acc:0.486 ===\n",
      "train loss:1.4672151679485868\n",
      "train loss:1.4493703718283029\n",
      "train loss:1.4436039896276116\n",
      "train loss:1.4797073582012399\n",
      "train loss:1.3870823055476\n",
      "train loss:1.4344812846028079\n",
      "train loss:1.4477184159499308\n",
      "train loss:1.4068148086256624\n",
      "train loss:1.4422150973775956\n",
      "train loss:1.4872874047215452\n",
      "=== epoch:35, train acc:0.685, test acc:0.492 ===\n",
      "train loss:1.4142114648925466\n",
      "train loss:1.3905377376294974\n",
      "train loss:1.4600590758265186\n",
      "train loss:1.3583271023467218\n",
      "train loss:1.386160059058532\n",
      "train loss:1.4259673253123446\n",
      "train loss:1.4094112812441717\n",
      "train loss:1.3784660135572089\n",
      "train loss:1.4357416374710144\n",
      "train loss:1.4645381594411493\n",
      "=== epoch:36, train acc:0.697, test acc:0.5 ===\n",
      "train loss:1.4164436646875465\n",
      "train loss:1.3920843601497697\n",
      "train loss:1.4205762064522451\n",
      "train loss:1.371608817506012\n",
      "train loss:1.3727693874309983\n",
      "train loss:1.3641040890305118\n",
      "train loss:1.3399175595956314\n",
      "train loss:1.3266869891077118\n",
      "train loss:1.334628463574611\n",
      "train loss:1.4080651722197106\n",
      "=== epoch:37, train acc:0.711, test acc:0.518 ===\n",
      "train loss:1.4296274637811641\n",
      "train loss:1.3639084765690779\n",
      "train loss:1.354677652906236\n",
      "train loss:1.378417185233082\n",
      "train loss:1.2697231476278261\n",
      "train loss:1.3682295555019275\n",
      "train loss:1.3372296650611373\n",
      "train loss:1.3886059804539952\n",
      "train loss:1.3643244774426122\n",
      "train loss:1.4157963693226814\n",
      "=== epoch:38, train acc:0.726, test acc:0.528 ===\n",
      "train loss:1.2736722607474829\n",
      "train loss:1.392201697652586\n",
      "train loss:1.2779652958844683\n",
      "train loss:1.4650988768277464\n",
      "train loss:1.367162531120346\n",
      "train loss:1.292217026180697\n",
      "train loss:1.3100025851527177\n",
      "train loss:1.2798374032587525\n",
      "train loss:1.379614473509734\n",
      "train loss:1.3323903349924544\n",
      "=== epoch:39, train acc:0.734, test acc:0.53 ===\n",
      "train loss:1.4069427767347897\n",
      "train loss:1.2685276815721576\n",
      "train loss:1.3431728577813302\n",
      "train loss:1.3153390030129908\n",
      "train loss:1.2338308830842275\n",
      "train loss:1.3104002219566544\n",
      "train loss:1.305456066469634\n",
      "train loss:1.2699058926212663\n",
      "train loss:1.3059426807785344\n",
      "train loss:1.3313574188555273\n",
      "=== epoch:40, train acc:0.754, test acc:0.542 ===\n",
      "train loss:1.356380083753056\n",
      "train loss:1.2912868026843742\n",
      "train loss:1.3513020063232224\n",
      "train loss:1.2966080273587954\n",
      "train loss:1.3141550986744464\n",
      "train loss:1.216277013950713\n",
      "train loss:1.255655296516037\n",
      "train loss:1.196050616367757\n",
      "train loss:1.269587644244941\n",
      "train loss:1.195062954836117\n",
      "=== epoch:41, train acc:0.765, test acc:0.55 ===\n",
      "train loss:1.2271187533963828\n",
      "train loss:1.301157736867858\n",
      "train loss:1.2864646885424724\n",
      "train loss:1.2494647999871669\n",
      "train loss:1.2753780384273097\n",
      "train loss:1.2686606803159375\n",
      "train loss:1.2360625055485215\n",
      "train loss:1.2382508104017271\n",
      "train loss:1.2149164717968828\n",
      "train loss:1.2573590036257634\n",
      "=== epoch:42, train acc:0.769, test acc:0.56 ===\n",
      "train loss:1.198244916897398\n",
      "train loss:1.1744630738129662\n",
      "train loss:1.251758601303312\n",
      "train loss:1.146224143356842\n",
      "train loss:1.1375885705249615\n",
      "train loss:1.2770882711602038\n",
      "train loss:1.3034555386925404\n",
      "train loss:1.229876723234847\n",
      "train loss:1.1652767965549822\n",
      "train loss:1.1933609794876088\n",
      "=== epoch:43, train acc:0.778, test acc:0.564 ===\n",
      "train loss:1.14807038059066\n",
      "train loss:1.1204971923618532\n",
      "train loss:1.2433920448792128\n",
      "train loss:1.1928269602402797\n",
      "train loss:1.2467531898797402\n",
      "train loss:1.1932337290279371\n",
      "train loss:1.2024384602881377\n",
      "train loss:1.2073219686016083\n",
      "train loss:1.1910103613071437\n",
      "train loss:1.2133208754336082\n",
      "=== epoch:44, train acc:0.785, test acc:0.578 ===\n",
      "train loss:1.1507252056892414\n",
      "train loss:1.1695971830344538\n",
      "train loss:1.13775488289147\n",
      "train loss:1.2153723937957979\n",
      "train loss:1.1261823975826781\n",
      "train loss:1.118034285608025\n",
      "train loss:1.2558591193528776\n",
      "train loss:1.10399939757213\n",
      "train loss:1.1689096835235733\n",
      "train loss:1.1796017316710516\n",
      "=== epoch:45, train acc:0.791, test acc:0.59 ===\n",
      "train loss:1.0816466515489411\n",
      "train loss:1.2035278569120742\n",
      "train loss:1.1428954922662724\n",
      "train loss:1.1139476954669763\n",
      "train loss:1.1853659532823762\n",
      "train loss:1.128590145395571\n",
      "train loss:1.1709310277325307\n",
      "train loss:1.1837342201234922\n",
      "train loss:1.0552361870515208\n",
      "train loss:1.1937086682278546\n",
      "=== epoch:46, train acc:0.795, test acc:0.598 ===\n",
      "train loss:1.1065689428672798\n",
      "train loss:1.0973094643565502\n",
      "train loss:1.1773064679156477\n",
      "train loss:1.1696899438001827\n",
      "train loss:1.097738647720467\n",
      "train loss:1.101738811233797\n",
      "train loss:1.118188772888201\n",
      "train loss:1.0323946315127612\n",
      "train loss:1.0204517481963797\n",
      "train loss:1.193774825518049\n",
      "=== epoch:47, train acc:0.803, test acc:0.606 ===\n",
      "train loss:1.0700664525680563\n",
      "train loss:1.114638096627136\n",
      "train loss:1.1148527058269757\n",
      "train loss:1.0088423600888503\n",
      "train loss:1.1134518974500287\n",
      "train loss:1.0461151317675443\n",
      "train loss:1.047393614277488\n",
      "train loss:1.1133682862809249\n",
      "train loss:1.188533063709217\n",
      "train loss:1.0834752974105877\n",
      "=== epoch:48, train acc:0.808, test acc:0.614 ===\n",
      "train loss:1.090288395291472\n",
      "train loss:0.9986938467808786\n",
      "train loss:1.1162227157816806\n",
      "train loss:1.0868017221237267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.12919297127595\n",
      "train loss:0.993916344096315\n",
      "train loss:0.9916673859716907\n",
      "train loss:1.0279143185217514\n",
      "train loss:1.0175282671036194\n",
      "train loss:1.0969394835074873\n",
      "=== epoch:49, train acc:0.813, test acc:0.63 ===\n",
      "train loss:1.0043242541974586\n",
      "train loss:1.1154855525244685\n",
      "train loss:1.0438401646937208\n",
      "train loss:1.0098202580522952\n",
      "train loss:0.9498614339624316\n",
      "train loss:1.1539088247695373\n",
      "train loss:1.0272882327581925\n",
      "train loss:1.0165552357801046\n",
      "train loss:1.0349689083233238\n",
      "train loss:1.0406144276737708\n",
      "=== epoch:50, train acc:0.817, test acc:0.638 ===\n",
      "train loss:1.0543033968977165\n",
      "train loss:1.049618002696628\n",
      "train loss:1.0386734996140883\n",
      "train loss:1.0767355836215158\n",
      "train loss:1.0048127933099764\n",
      "train loss:1.0817584241649876\n",
      "train loss:0.9026850932551556\n",
      "train loss:0.9766263424571382\n",
      "train loss:1.033434830126864\n",
      "train loss:0.958318537215559\n",
      "=== epoch:51, train acc:0.825, test acc:0.644 ===\n",
      "train loss:0.9809167353609157\n",
      "train loss:1.0430101056039047\n",
      "train loss:0.9832124057723871\n",
      "train loss:0.9292923664915469\n",
      "train loss:0.9851539627792111\n",
      "train loss:0.9923590827814549\n",
      "train loss:1.0233239826067828\n",
      "train loss:0.9355927677538999\n",
      "train loss:0.9217757818610537\n",
      "train loss:0.9965476882128503\n",
      "=== epoch:52, train acc:0.831, test acc:0.656 ===\n",
      "train loss:0.9808247123534427\n",
      "train loss:1.0170967916641986\n",
      "train loss:1.0072799377672057\n",
      "train loss:0.9634425766810133\n",
      "train loss:1.048595238046772\n",
      "train loss:1.0298994583493064\n",
      "train loss:0.9656161376511717\n",
      "train loss:1.025989779015559\n",
      "train loss:1.0296335040773852\n",
      "train loss:0.9285855889945951\n",
      "=== epoch:53, train acc:0.836, test acc:0.66 ===\n",
      "train loss:0.9778059624857566\n",
      "train loss:0.9583881128636291\n",
      "train loss:0.9402401810123113\n",
      "train loss:0.9001310231220814\n",
      "train loss:0.8472315739877879\n",
      "train loss:0.9817458872202096\n",
      "train loss:1.0719025509146654\n",
      "train loss:0.9809297199957012\n",
      "train loss:0.9723022763855137\n",
      "train loss:0.8964390385557551\n",
      "=== epoch:54, train acc:0.845, test acc:0.666 ===\n",
      "train loss:1.0222726254507184\n",
      "train loss:0.9307760827061112\n",
      "train loss:0.8194612445475571\n",
      "train loss:0.8700886572473681\n",
      "train loss:0.9326860008866816\n",
      "train loss:0.9217041223765408\n",
      "train loss:0.9664997533903888\n",
      "train loss:0.9534996470612904\n",
      "train loss:0.840871628252962\n",
      "train loss:0.8740161710912199\n",
      "=== epoch:55, train acc:0.846, test acc:0.676 ===\n",
      "train loss:0.8491791536246134\n",
      "train loss:0.9281404610346857\n",
      "train loss:0.9565100003109993\n",
      "train loss:0.9001323146711608\n",
      "train loss:0.9348842605487472\n",
      "train loss:0.8732701968779294\n",
      "train loss:0.9574921640475499\n",
      "train loss:0.9015347138946441\n",
      "train loss:0.9186102144309903\n",
      "train loss:0.868744420094065\n",
      "=== epoch:56, train acc:0.852, test acc:0.684 ===\n",
      "train loss:0.8933006573206999\n",
      "train loss:0.9298696857291621\n",
      "train loss:0.8690874829334565\n",
      "train loss:0.84994135183873\n",
      "train loss:0.7656962165421629\n",
      "train loss:0.8252055956775076\n",
      "train loss:0.8749469531533642\n",
      "train loss:0.8641064844166253\n",
      "train loss:0.9162609296630686\n",
      "train loss:0.9492112026611514\n",
      "=== epoch:57, train acc:0.856, test acc:0.678 ===\n",
      "train loss:0.8223435558099758\n",
      "train loss:0.8032567880701091\n",
      "train loss:0.8324506584276018\n",
      "train loss:0.7609129567720119\n",
      "train loss:0.858160473185371\n",
      "train loss:0.8278485138762884\n",
      "train loss:0.9111181095743791\n",
      "train loss:0.8607950213041612\n",
      "train loss:0.7658508850238797\n",
      "train loss:0.8185202689514258\n",
      "=== epoch:58, train acc:0.863, test acc:0.68 ===\n",
      "train loss:0.9060247823129909\n",
      "train loss:0.9128121269533481\n",
      "train loss:0.8798128275124568\n",
      "train loss:0.8948621313130929\n",
      "train loss:0.8833720990979559\n",
      "train loss:0.7790472625388704\n",
      "train loss:0.7523190428965211\n",
      "train loss:0.8569488057339043\n",
      "train loss:0.895856431475789\n",
      "train loss:0.7912170720746018\n",
      "=== epoch:59, train acc:0.865, test acc:0.7 ===\n",
      "train loss:0.8165975480399831\n",
      "train loss:0.8474529482186166\n",
      "train loss:0.8153370142061059\n",
      "train loss:0.792650997043707\n",
      "train loss:0.8347751603573298\n",
      "train loss:0.7784632323833698\n",
      "train loss:0.8141841397798832\n",
      "train loss:0.8038727746789405\n",
      "train loss:0.7204289837758323\n",
      "train loss:0.8436450337260761\n",
      "=== epoch:60, train acc:0.869, test acc:0.702 ===\n",
      "train loss:0.8255640167160216\n",
      "train loss:0.7722944098420453\n",
      "train loss:0.7384117888626451\n",
      "train loss:0.8661916733918951\n",
      "train loss:0.8270356755484386\n",
      "train loss:0.7893818317032759\n",
      "train loss:0.7641826036557725\n",
      "train loss:0.765689376926177\n",
      "train loss:0.8326205184242126\n",
      "train loss:0.668841887188657\n",
      "=== epoch:61, train acc:0.875, test acc:0.712 ===\n",
      "train loss:0.7396603404394487\n",
      "train loss:0.6737447076758459\n",
      "train loss:0.8232922345173275\n",
      "train loss:0.8948339415258476\n",
      "train loss:0.7236961268917771\n",
      "train loss:0.7877803841565327\n",
      "train loss:0.8254709850248798\n",
      "train loss:0.7639903865155382\n",
      "train loss:0.7592580605839452\n",
      "train loss:0.6891040685341053\n",
      "=== epoch:62, train acc:0.877, test acc:0.716 ===\n",
      "train loss:0.6912126931924627\n",
      "train loss:0.8020187533497791\n",
      "train loss:0.6425145908444213\n",
      "train loss:0.7354395243671599\n",
      "train loss:0.7978319988854982\n",
      "train loss:0.7584868882661142\n",
      "train loss:0.6890645695937125\n",
      "train loss:0.707012169254886\n",
      "train loss:0.7851104889820293\n",
      "train loss:0.6769163779540429\n",
      "=== epoch:63, train acc:0.882, test acc:0.726 ===\n",
      "train loss:0.7693101890221805\n",
      "train loss:0.8086295373886095\n",
      "train loss:0.7253157652941191\n",
      "train loss:0.7672464128984232\n",
      "train loss:0.7782998205378374\n",
      "train loss:0.7102241695838724\n",
      "train loss:0.7803779882698264\n",
      "train loss:0.6698992724157392\n",
      "train loss:0.6644916887410198\n",
      "train loss:0.6954624475563038\n",
      "=== epoch:64, train acc:0.887, test acc:0.732 ===\n",
      "train loss:0.7879200104578267\n",
      "train loss:0.8445771213845441\n",
      "train loss:0.6825031675411892\n",
      "train loss:0.6993388564478741\n",
      "train loss:0.6813909682280166\n",
      "train loss:0.7037468950547201\n",
      "train loss:0.6973873948284036\n",
      "train loss:0.6684596566981097\n",
      "train loss:0.7263179431882321\n",
      "train loss:0.6880916325360014\n",
      "=== epoch:65, train acc:0.891, test acc:0.734 ===\n",
      "train loss:0.7429348814756581\n",
      "train loss:0.7884053361400484\n",
      "train loss:0.6472966367942399\n",
      "train loss:0.6221509750866463\n",
      "train loss:0.6532695233118195\n",
      "train loss:0.770237191769058\n",
      "train loss:0.6846048496831384\n",
      "train loss:0.7636319802949265\n",
      "train loss:0.730954791228703\n",
      "train loss:0.7617362071498013\n",
      "=== epoch:66, train acc:0.894, test acc:0.738 ===\n",
      "train loss:0.6866455550468674\n",
      "train loss:0.6678092077414466\n",
      "train loss:0.6188685838958791\n",
      "train loss:0.6889604705972248\n",
      "train loss:0.6899868953638131\n",
      "train loss:0.6759849967706164\n",
      "train loss:0.7448334162768788\n",
      "train loss:0.8045641007722211\n",
      "train loss:0.6107387568470972\n",
      "train loss:0.6666323621964836\n",
      "=== epoch:67, train acc:0.895, test acc:0.74 ===\n",
      "train loss:0.5897920064608487\n",
      "train loss:0.6800523994558254\n",
      "train loss:0.6880899584244332\n",
      "train loss:0.7773366907215533\n",
      "train loss:0.7130540517990411\n",
      "train loss:0.6902711167574774\n",
      "train loss:0.6840300508611495\n",
      "train loss:0.7305571789572296\n",
      "train loss:0.6470930153148884\n",
      "train loss:0.6785229883803109\n",
      "=== epoch:68, train acc:0.892, test acc:0.742 ===\n",
      "train loss:0.6975038167591933\n",
      "train loss:0.644532598099318\n",
      "train loss:0.7291695463304442\n",
      "train loss:0.6125959526503518\n",
      "train loss:0.6128588779407484\n",
      "train loss:0.6481392850179729\n",
      "train loss:0.612906197503248\n",
      "train loss:0.6536722303128818\n",
      "train loss:0.600503703033024\n",
      "train loss:0.5741714474231261\n",
      "=== epoch:69, train acc:0.895, test acc:0.746 ===\n",
      "train loss:0.6440424934380183\n",
      "train loss:0.6739533213784468\n",
      "train loss:0.5892809263510643\n",
      "train loss:0.617461660513644\n",
      "train loss:0.6853834873315298\n",
      "train loss:0.6166558879032522\n",
      "train loss:0.6813008937501348\n",
      "train loss:0.6823525000127069\n",
      "train loss:0.6935618504388813\n",
      "train loss:0.6593187764239115\n",
      "=== epoch:70, train acc:0.898, test acc:0.744 ===\n",
      "train loss:0.5653698455752727\n",
      "train loss:0.7085246562709606\n",
      "train loss:0.5999487021894423\n",
      "train loss:0.6620073385833486\n",
      "train loss:0.649892352464665\n",
      "train loss:0.7194779650582549\n",
      "train loss:0.6119133209995005\n",
      "train loss:0.6132135598135858\n",
      "train loss:0.5786173601719145\n",
      "train loss:0.5637534825499778\n",
      "=== epoch:71, train acc:0.901, test acc:0.752 ===\n",
      "train loss:0.6265254006118526\n",
      "train loss:0.5711001903600542\n",
      "train loss:0.5854309610419517\n",
      "train loss:0.7240866302900958\n",
      "train loss:0.5918888338647403\n",
      "train loss:0.6241412542181597\n",
      "train loss:0.6145967233435804\n",
      "train loss:0.6588996181464789\n",
      "train loss:0.6619299358644235\n",
      "train loss:0.6040845754997913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:72, train acc:0.901, test acc:0.754 ===\n",
      "train loss:0.6314801364540478\n",
      "train loss:0.6034507188844986\n",
      "train loss:0.6622011450861547\n",
      "train loss:0.6242640784930863\n",
      "train loss:0.5372370166638448\n",
      "train loss:0.5103594010272161\n",
      "train loss:0.6693592920334908\n",
      "train loss:0.6585361829599364\n",
      "train loss:0.5610021590517681\n",
      "train loss:0.5706708003117245\n",
      "=== epoch:73, train acc:0.903, test acc:0.754 ===\n",
      "train loss:0.5888530399015371\n",
      "train loss:0.6042879039773257\n",
      "train loss:0.5500475102309661\n",
      "train loss:0.5007901459117862\n",
      "train loss:0.5962098416219828\n",
      "train loss:0.5616418563273409\n",
      "train loss:0.5795197770122532\n",
      "train loss:0.5789139837561772\n",
      "train loss:0.5109112304290904\n",
      "train loss:0.6372567183985004\n",
      "=== epoch:74, train acc:0.911, test acc:0.756 ===\n",
      "train loss:0.4806033470839154\n",
      "train loss:0.542804599848831\n",
      "train loss:0.5845424848184682\n",
      "train loss:0.5613057715203209\n",
      "train loss:0.4774551928107031\n",
      "train loss:0.5537097148744963\n",
      "train loss:0.5439757598848037\n",
      "train loss:0.5594735401484652\n",
      "train loss:0.5109113001021188\n",
      "train loss:0.5683133280133705\n",
      "=== epoch:75, train acc:0.91, test acc:0.756 ===\n",
      "train loss:0.6120281045185272\n",
      "train loss:0.5300065783861151\n",
      "train loss:0.5170082514236031\n",
      "train loss:0.5029797658390942\n",
      "train loss:0.5237754052113778\n",
      "train loss:0.5899683275033535\n",
      "train loss:0.5498344891495178\n",
      "train loss:0.5545055945461087\n",
      "train loss:0.6048381810122876\n",
      "train loss:0.5294148396417059\n",
      "=== epoch:76, train acc:0.913, test acc:0.756 ===\n",
      "train loss:0.5427339927487816\n",
      "train loss:0.6188991643160507\n",
      "train loss:0.5510147986366001\n",
      "train loss:0.5363774598150609\n",
      "train loss:0.6139737581278898\n",
      "train loss:0.5063934361138727\n",
      "train loss:0.5373345604152057\n",
      "train loss:0.5531930720286299\n",
      "train loss:0.6486454406775857\n",
      "train loss:0.4767212745385475\n",
      "=== epoch:77, train acc:0.916, test acc:0.758 ===\n",
      "train loss:0.4932684253393785\n",
      "train loss:0.5022880327230752\n",
      "train loss:0.4820904494119794\n",
      "train loss:0.5523279884075771\n",
      "train loss:0.5524900562513148\n",
      "train loss:0.5445485319375838\n",
      "train loss:0.5657290420624883\n",
      "train loss:0.5307539421269108\n",
      "train loss:0.5515703694130917\n",
      "train loss:0.574325908270929\n",
      "=== epoch:78, train acc:0.918, test acc:0.764 ===\n",
      "train loss:0.49459403514197986\n",
      "train loss:0.5165543679445391\n",
      "train loss:0.5970722065316687\n",
      "train loss:0.5014874864704982\n",
      "train loss:0.5440076715112777\n",
      "train loss:0.4924253875012196\n",
      "train loss:0.5993162316029443\n",
      "train loss:0.49803846342252883\n",
      "train loss:0.516313004531103\n",
      "train loss:0.49418012154666285\n",
      "=== epoch:79, train acc:0.917, test acc:0.768 ===\n",
      "train loss:0.4603051085252526\n",
      "train loss:0.5212046802207534\n",
      "train loss:0.4515621837062038\n",
      "train loss:0.4469789485610756\n",
      "train loss:0.501868729023888\n",
      "train loss:0.5005712135743882\n",
      "train loss:0.5822123813290614\n",
      "train loss:0.5479093750082479\n",
      "train loss:0.5486282313533964\n",
      "train loss:0.5439520784708592\n",
      "=== epoch:80, train acc:0.923, test acc:0.77 ===\n",
      "train loss:0.5221515531233488\n",
      "train loss:0.4641424566739953\n",
      "train loss:0.4722458740540075\n",
      "train loss:0.5334080868278551\n",
      "train loss:0.5094276208273147\n",
      "train loss:0.46886026559859956\n",
      "train loss:0.4729575208002995\n",
      "train loss:0.48091749439149994\n",
      "train loss:0.49628966833224686\n",
      "train loss:0.5287809416340852\n",
      "=== epoch:81, train acc:0.925, test acc:0.772 ===\n",
      "train loss:0.4935604945584574\n",
      "train loss:0.5371176402577454\n",
      "train loss:0.46011733239432545\n",
      "train loss:0.587910723288024\n",
      "train loss:0.5126319108238278\n",
      "train loss:0.461330483670538\n",
      "train loss:0.4681773162905447\n",
      "train loss:0.5637061399773511\n",
      "train loss:0.4769416506074913\n",
      "train loss:0.4729922188371675\n",
      "=== epoch:82, train acc:0.926, test acc:0.778 ===\n",
      "train loss:0.4445909049073737\n",
      "train loss:0.4829172016922214\n",
      "train loss:0.5163186456477206\n",
      "train loss:0.4686568246523656\n",
      "train loss:0.5321338599241063\n",
      "train loss:0.48020706388877427\n",
      "train loss:0.4800317177002193\n",
      "train loss:0.4246428649117437\n",
      "train loss:0.5208024849456055\n",
      "train loss:0.4671049168579936\n",
      "=== epoch:83, train acc:0.928, test acc:0.774 ===\n",
      "train loss:0.49240340106162045\n",
      "train loss:0.47557267414071175\n",
      "train loss:0.3928262345950545\n",
      "train loss:0.4593514856629462\n",
      "train loss:0.47129326157021056\n",
      "train loss:0.42858327348706615\n",
      "train loss:0.4638663968986556\n",
      "train loss:0.36627466307054646\n",
      "train loss:0.4567346096063074\n",
      "train loss:0.43575449128204236\n",
      "=== epoch:84, train acc:0.929, test acc:0.776 ===\n",
      "train loss:0.43586599005753834\n",
      "train loss:0.45675051263123156\n",
      "train loss:0.4908817187444183\n",
      "train loss:0.4065028180147654\n",
      "train loss:0.43603615144837\n",
      "train loss:0.4777712507885804\n",
      "train loss:0.4080023855863781\n",
      "train loss:0.47308405793858815\n",
      "train loss:0.41896429532559454\n",
      "train loss:0.5468238397913555\n",
      "=== epoch:85, train acc:0.929, test acc:0.776 ===\n",
      "train loss:0.5115633324379029\n",
      "train loss:0.4187381586114604\n",
      "train loss:0.5078039656222392\n",
      "train loss:0.43124635996113925\n",
      "train loss:0.38530828938752143\n",
      "train loss:0.5343945752409662\n",
      "train loss:0.5064810564985673\n",
      "train loss:0.5343994735725918\n",
      "train loss:0.45667817060790983\n",
      "train loss:0.48805879152252646\n",
      "=== epoch:86, train acc:0.929, test acc:0.786 ===\n",
      "train loss:0.4743734131814704\n",
      "train loss:0.41328316796502734\n",
      "train loss:0.5026988825958221\n",
      "train loss:0.4302152103668946\n",
      "train loss:0.4969189135462719\n",
      "train loss:0.399462612754623\n",
      "train loss:0.3877169800091697\n",
      "train loss:0.4248982025161688\n",
      "train loss:0.45365098831328354\n",
      "train loss:0.3944396845965972\n",
      "=== epoch:87, train acc:0.93, test acc:0.784 ===\n",
      "train loss:0.4048922353759323\n",
      "train loss:0.40904745984286456\n",
      "train loss:0.3355310917965722\n",
      "train loss:0.4116111003101503\n",
      "train loss:0.4451615986115939\n",
      "train loss:0.4436467869964803\n",
      "train loss:0.444354258220512\n",
      "train loss:0.45404407587566153\n",
      "train loss:0.5341346319594448\n",
      "train loss:0.4329974263016259\n",
      "=== epoch:88, train acc:0.931, test acc:0.79 ===\n",
      "train loss:0.45669192925976965\n",
      "train loss:0.4148628600593145\n",
      "train loss:0.4176307686262774\n",
      "train loss:0.42933203098164746\n",
      "train loss:0.41393635346399343\n",
      "train loss:0.46427903106208546\n",
      "train loss:0.4035840470192247\n",
      "train loss:0.4385064858192247\n",
      "train loss:0.4566785984632689\n",
      "train loss:0.47461060663543575\n",
      "=== epoch:89, train acc:0.934, test acc:0.786 ===\n",
      "train loss:0.3850652417372583\n",
      "train loss:0.3942310053031116\n",
      "train loss:0.4042029263877583\n",
      "train loss:0.3098894906154538\n",
      "train loss:0.36711556307057536\n",
      "train loss:0.441718089322893\n",
      "train loss:0.4013971232597453\n",
      "train loss:0.3980285043235494\n",
      "train loss:0.36218147836805187\n",
      "train loss:0.34178087419312364\n",
      "=== epoch:90, train acc:0.937, test acc:0.786 ===\n",
      "train loss:0.43802105171878736\n",
      "train loss:0.4354361869123916\n",
      "train loss:0.32900279622091894\n",
      "train loss:0.37561442678369095\n",
      "train loss:0.4303667040854978\n",
      "train loss:0.4092067238393331\n",
      "train loss:0.3318781188244149\n",
      "train loss:0.38030074439719014\n",
      "train loss:0.41777465412268355\n",
      "train loss:0.449436905824656\n",
      "=== epoch:91, train acc:0.94, test acc:0.792 ===\n",
      "train loss:0.3840996392765606\n",
      "train loss:0.37404379263536347\n",
      "train loss:0.3782391809244509\n",
      "train loss:0.4358759823789993\n",
      "train loss:0.31555457938185005\n",
      "train loss:0.46753713465754915\n",
      "train loss:0.391792767018787\n",
      "train loss:0.41127509225195524\n",
      "train loss:0.47137944157000866\n",
      "train loss:0.30769327133324237\n",
      "=== epoch:92, train acc:0.939, test acc:0.798 ===\n",
      "train loss:0.365903170035714\n",
      "train loss:0.40159418918095346\n",
      "train loss:0.39707688765208404\n",
      "train loss:0.49962231666855034\n",
      "train loss:0.398323478995161\n",
      "train loss:0.3572673914458552\n",
      "train loss:0.3638982087569987\n",
      "train loss:0.34077566471523374\n",
      "train loss:0.3459311698474\n",
      "train loss:0.3977247753358612\n",
      "=== epoch:93, train acc:0.94, test acc:0.806 ===\n",
      "train loss:0.40270630372775507\n",
      "train loss:0.37536655668446367\n",
      "train loss:0.38714336502983904\n",
      "train loss:0.34538558654194695\n",
      "train loss:0.37321620119916893\n",
      "train loss:0.4149310012370379\n",
      "train loss:0.35130835417561934\n",
      "train loss:0.4141289556057047\n",
      "train loss:0.28895828870847323\n",
      "train loss:0.39910752148129036\n",
      "=== epoch:94, train acc:0.942, test acc:0.804 ===\n",
      "train loss:0.34054000617907687\n",
      "train loss:0.41664149636710635\n",
      "train loss:0.4300818347569191\n",
      "train loss:0.34670676483561175\n",
      "train loss:0.32632041714849547\n",
      "train loss:0.4216434843697426\n",
      "train loss:0.342400643379577\n",
      "train loss:0.3603983948741569\n",
      "train loss:0.32116726263473067\n",
      "train loss:0.3371716087899692\n",
      "=== epoch:95, train acc:0.942, test acc:0.798 ===\n",
      "train loss:0.3756956457216095\n",
      "train loss:0.3381353112731368\n",
      "train loss:0.34733380544403614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.3533167554790378\n",
      "train loss:0.41942422757618225\n",
      "train loss:0.3633623109545785\n",
      "train loss:0.3167075907846542\n",
      "train loss:0.39292050771166964\n",
      "train loss:0.3108877382827158\n",
      "train loss:0.4080892608032227\n",
      "=== epoch:96, train acc:0.943, test acc:0.8 ===\n",
      "train loss:0.3986091674131769\n",
      "train loss:0.35760440256494763\n",
      "train loss:0.43480547912238415\n",
      "train loss:0.4230568383711201\n",
      "train loss:0.36609856186559414\n",
      "train loss:0.34759175474944626\n",
      "train loss:0.3425352904182569\n",
      "train loss:0.4253452186033186\n",
      "train loss:0.3588402290702915\n",
      "train loss:0.3521378839992055\n",
      "=== epoch:97, train acc:0.944, test acc:0.8 ===\n",
      "train loss:0.3762283526356622\n",
      "train loss:0.2887799082891053\n",
      "train loss:0.4011601463204966\n",
      "train loss:0.35045297195321423\n",
      "train loss:0.389304904041567\n",
      "train loss:0.4089768842219674\n",
      "train loss:0.3484359689105605\n",
      "train loss:0.3730901777731265\n",
      "train loss:0.35423781364136253\n",
      "train loss:0.3313297610052648\n",
      "=== epoch:98, train acc:0.945, test acc:0.806 ===\n",
      "train loss:0.2569312079432376\n",
      "train loss:0.3566226337156123\n",
      "train loss:0.28733639060212357\n",
      "train loss:0.3836461866423156\n",
      "train loss:0.31179422926487876\n",
      "train loss:0.378953188742768\n",
      "train loss:0.3822978195372263\n",
      "train loss:0.35379932701432787\n",
      "train loss:0.3569339407761567\n",
      "train loss:0.32379936646376867\n",
      "=== epoch:99, train acc:0.945, test acc:0.806 ===\n",
      "train loss:0.3820164504466531\n",
      "train loss:0.42037772995993294\n",
      "train loss:0.3026904926172767\n",
      "train loss:0.35371875844787604\n",
      "train loss:0.33411820296873707\n",
      "train loss:0.34663898083293987\n",
      "train loss:0.31343316219787765\n",
      "train loss:0.36773118890460543\n",
      "train loss:0.35121682462291637\n",
      "train loss:0.36691523382263513\n",
      "=== epoch:100, train acc:0.946, test acc:0.802 ===\n",
      "train loss:0.3654248374790643\n",
      "train loss:0.36019320374429653\n",
      "train loss:0.32710452364184633\n",
      "train loss:0.36573822433747216\n",
      "train loss:0.28235321593624296\n",
      "train loss:0.3404541056257152\n",
      "train loss:0.3232812742042952\n",
      "train loss:0.4156006223728825\n",
      "train loss:0.3399552742628709\n",
      "train loss:0.3078390535570863\n",
      "=== epoch:101, train acc:0.946, test acc:0.812 ===\n",
      "train loss:0.3376840047118872\n",
      "train loss:0.36726786799211397\n",
      "train loss:0.4118858714770524\n",
      "train loss:0.4051899764202953\n",
      "train loss:0.37619015743633105\n",
      "train loss:0.3348263272348154\n",
      "train loss:0.36640946427648446\n",
      "train loss:0.3491574474278174\n",
      "train loss:0.2919677470623682\n",
      "train loss:0.34001852807619626\n",
      "=== epoch:102, train acc:0.946, test acc:0.812 ===\n",
      "train loss:0.328983556101371\n",
      "train loss:0.36789663941920653\n",
      "train loss:0.30063762081453116\n",
      "train loss:0.34853630345091674\n",
      "train loss:0.28140173666614865\n",
      "train loss:0.40352509717691964\n",
      "train loss:0.3622067775077051\n",
      "train loss:0.304161737327457\n",
      "train loss:0.3127695952814208\n",
      "train loss:0.29839734720104616\n",
      "=== epoch:103, train acc:0.946, test acc:0.814 ===\n",
      "train loss:0.3529146680254059\n",
      "train loss:0.35768058844620804\n",
      "train loss:0.38368852693557864\n",
      "train loss:0.31091624652208244\n",
      "train loss:0.28649069979394826\n",
      "train loss:0.2979374079509156\n",
      "train loss:0.31040650234431455\n",
      "train loss:0.3297742878111151\n",
      "train loss:0.3306062593074588\n",
      "train loss:0.330147370200301\n",
      "=== epoch:104, train acc:0.946, test acc:0.82 ===\n",
      "train loss:0.2887734475028547\n",
      "train loss:0.29540649786441686\n",
      "train loss:0.32564106710620344\n",
      "train loss:0.3131394480297528\n",
      "train loss:0.32617401773693244\n",
      "train loss:0.30033346831190383\n",
      "train loss:0.38283243691827606\n",
      "train loss:0.23536705114256462\n",
      "train loss:0.31131282836537627\n",
      "train loss:0.332447968883232\n",
      "=== epoch:105, train acc:0.949, test acc:0.814 ===\n",
      "train loss:0.2871095996946104\n",
      "train loss:0.31785624771820925\n",
      "train loss:0.31296548848823535\n",
      "train loss:0.3405505157144396\n",
      "train loss:0.36824134070955566\n",
      "train loss:0.29925096458257167\n",
      "train loss:0.294872459720907\n",
      "train loss:0.2443436511430847\n",
      "train loss:0.25697820362207\n",
      "train loss:0.3093507800391023\n",
      "=== epoch:106, train acc:0.948, test acc:0.818 ===\n",
      "train loss:0.3527575549730375\n",
      "train loss:0.2735534885775459\n",
      "train loss:0.31752208098581847\n",
      "train loss:0.3167513856926067\n",
      "train loss:0.2972942745201276\n",
      "train loss:0.366778579232413\n",
      "train loss:0.3219800479376513\n",
      "train loss:0.3095909297004009\n",
      "train loss:0.28958532809585025\n",
      "train loss:0.3186149592900193\n",
      "=== epoch:107, train acc:0.95, test acc:0.818 ===\n",
      "train loss:0.29077605046657173\n",
      "train loss:0.2842813761983839\n",
      "train loss:0.2580907891651859\n",
      "train loss:0.27973671911746445\n",
      "train loss:0.2737941041735118\n",
      "train loss:0.33672468649604587\n",
      "train loss:0.2930179410486027\n",
      "train loss:0.3603514558052093\n",
      "train loss:0.2626755105853057\n",
      "train loss:0.3047999208338325\n",
      "=== epoch:108, train acc:0.952, test acc:0.816 ===\n",
      "train loss:0.29884690880381604\n",
      "train loss:0.32819969460919457\n",
      "train loss:0.30499921816632053\n",
      "train loss:0.31207098731145017\n",
      "train loss:0.2840028431335749\n",
      "train loss:0.30008489925227805\n",
      "train loss:0.2885300336653513\n",
      "train loss:0.23252160299382413\n",
      "train loss:0.26521748850130006\n",
      "train loss:0.29493881950138334\n",
      "=== epoch:109, train acc:0.953, test acc:0.818 ===\n",
      "train loss:0.32909073988091125\n",
      "train loss:0.24237268358547\n",
      "train loss:0.32512648356793616\n",
      "train loss:0.25212900743780575\n",
      "train loss:0.36670631660379066\n",
      "train loss:0.3254157393248585\n",
      "train loss:0.3025587781358245\n",
      "train loss:0.24585556683966925\n",
      "train loss:0.32550768808005415\n",
      "train loss:0.2598041467262112\n",
      "=== epoch:110, train acc:0.952, test acc:0.82 ===\n",
      "train loss:0.31865522387325657\n",
      "train loss:0.24107821938127605\n",
      "train loss:0.2370468018603108\n",
      "train loss:0.3692861146837416\n",
      "train loss:0.3150893417529173\n",
      "train loss:0.2746363294397476\n",
      "train loss:0.31610472094781245\n",
      "train loss:0.2641694464290702\n",
      "train loss:0.3145921260605315\n",
      "train loss:0.27142480512289635\n",
      "=== epoch:111, train acc:0.952, test acc:0.82 ===\n",
      "train loss:0.23690204508994497\n",
      "train loss:0.3588221851563854\n",
      "train loss:0.29739866590020114\n",
      "train loss:0.20538475705821457\n",
      "train loss:0.30298207722493614\n",
      "train loss:0.2737034019612347\n",
      "train loss:0.2761413961352006\n",
      "train loss:0.2857696280353841\n",
      "train loss:0.31718822513567696\n",
      "train loss:0.246658964184678\n",
      "=== epoch:112, train acc:0.954, test acc:0.824 ===\n",
      "train loss:0.2769740004959933\n",
      "train loss:0.31496321361991375\n",
      "train loss:0.3093126426416336\n",
      "train loss:0.2375684987512359\n",
      "train loss:0.28151511682577957\n",
      "train loss:0.2567049794862581\n",
      "train loss:0.3424935999881475\n",
      "train loss:0.3327493024731267\n",
      "train loss:0.2742904841584604\n",
      "train loss:0.32573901132578953\n",
      "=== epoch:113, train acc:0.955, test acc:0.824 ===\n",
      "train loss:0.3342868408883731\n",
      "train loss:0.24685112689683497\n",
      "train loss:0.3329623129432935\n",
      "train loss:0.310547411195378\n",
      "train loss:0.2954163016937643\n",
      "train loss:0.23769587432182238\n",
      "train loss:0.24411336459120797\n",
      "train loss:0.2740505171063615\n",
      "train loss:0.29094043331731007\n",
      "train loss:0.2150423657580604\n",
      "=== epoch:114, train acc:0.957, test acc:0.824 ===\n",
      "train loss:0.3237135642108315\n",
      "train loss:0.23162813327290638\n",
      "train loss:0.24058413184946262\n",
      "train loss:0.32104745174436267\n",
      "train loss:0.30468250058712815\n",
      "train loss:0.2703621716803751\n",
      "train loss:0.2945122499195137\n",
      "train loss:0.2831706347829232\n",
      "train loss:0.28230649901297544\n",
      "train loss:0.19944780234876838\n",
      "=== epoch:115, train acc:0.958, test acc:0.826 ===\n",
      "train loss:0.2962631162287331\n",
      "train loss:0.2909439366350656\n",
      "train loss:0.22535750344064248\n",
      "train loss:0.24309304196275833\n",
      "train loss:0.2600129214686667\n",
      "train loss:0.2717392911920764\n",
      "train loss:0.21134586682122966\n",
      "train loss:0.23020213614089355\n",
      "train loss:0.2556478788770527\n",
      "train loss:0.24844289219897817\n",
      "=== epoch:116, train acc:0.958, test acc:0.828 ===\n",
      "train loss:0.30232704406565614\n",
      "train loss:0.26859493086800046\n",
      "train loss:0.2781823939162994\n",
      "train loss:0.33112682154249334\n",
      "train loss:0.26377631402897606\n",
      "train loss:0.3343411051654509\n",
      "train loss:0.298362481961693\n",
      "train loss:0.22419253299397546\n",
      "train loss:0.3356766511465085\n",
      "train loss:0.27797643263936217\n",
      "=== epoch:117, train acc:0.959, test acc:0.83 ===\n",
      "train loss:0.242569858250122\n",
      "train loss:0.2702397716848319\n",
      "train loss:0.30338523818420726\n",
      "train loss:0.25860055071736804\n",
      "train loss:0.28573131826365317\n",
      "train loss:0.3401943757215391\n",
      "train loss:0.2531122659348923\n",
      "train loss:0.26072404944690847\n",
      "train loss:0.23987165555939277\n",
      "train loss:0.30933903209757796\n",
      "=== epoch:118, train acc:0.959, test acc:0.83 ===\n",
      "train loss:0.3053243553386672\n",
      "train loss:0.27112750548752024\n",
      "train loss:0.3483710257412263\n",
      "train loss:0.2670158074406496\n",
      "train loss:0.29162126181158626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2635987811574719\n",
      "train loss:0.30249308040982864\n",
      "train loss:0.22635796205586625\n",
      "train loss:0.2216125549547936\n",
      "train loss:0.27638923603675886\n",
      "=== epoch:119, train acc:0.959, test acc:0.834 ===\n",
      "train loss:0.2506763240644034\n",
      "train loss:0.3732650484709772\n",
      "train loss:0.20453769474164535\n",
      "train loss:0.25251807031456036\n",
      "train loss:0.3557086633979374\n",
      "train loss:0.2964437230108893\n",
      "train loss:0.29710761307142874\n",
      "train loss:0.2238539628508747\n",
      "train loss:0.25769025682813607\n",
      "train loss:0.3097124216532869\n",
      "=== epoch:120, train acc:0.959, test acc:0.834 ===\n",
      "train loss:0.3086480538906007\n",
      "train loss:0.23396434235343683\n",
      "train loss:0.23424521496600803\n",
      "train loss:0.2556353630531992\n",
      "train loss:0.2708815071787238\n",
      "train loss:0.2532117911884733\n",
      "train loss:0.34546173192240054\n",
      "train loss:0.259431212434728\n",
      "train loss:0.2994968540462316\n",
      "train loss:0.280301518177709\n",
      "=== epoch:121, train acc:0.961, test acc:0.836 ===\n",
      "train loss:0.23867201969780935\n",
      "train loss:0.2180449210127919\n",
      "train loss:0.21743557186401843\n",
      "train loss:0.25278608879165143\n",
      "train loss:0.2566434922429817\n",
      "train loss:0.2435068535211863\n",
      "train loss:0.317482675170022\n",
      "train loss:0.25002347142795994\n",
      "train loss:0.21418127046432797\n",
      "train loss:0.22777590546795703\n",
      "=== epoch:122, train acc:0.96, test acc:0.832 ===\n",
      "train loss:0.2381497955395887\n",
      "train loss:0.24115645632853464\n",
      "train loss:0.1890885279846833\n",
      "train loss:0.25065918246742247\n",
      "train loss:0.22638127090994994\n",
      "train loss:0.2678527401608375\n",
      "train loss:0.22326920049674664\n",
      "train loss:0.2747944050153848\n",
      "train loss:0.3388849002582036\n",
      "train loss:0.24017615831892822\n",
      "=== epoch:123, train acc:0.96, test acc:0.834 ===\n",
      "train loss:0.22924247399166248\n",
      "train loss:0.2141137131357159\n",
      "train loss:0.25011301449285833\n",
      "train loss:0.20171465483592022\n",
      "train loss:0.18859715549323308\n",
      "train loss:0.20114594486572435\n",
      "train loss:0.22902896946491463\n",
      "train loss:0.20292522910382513\n",
      "train loss:0.25394749284905527\n",
      "train loss:0.2215046977192755\n",
      "=== epoch:124, train acc:0.962, test acc:0.834 ===\n",
      "train loss:0.19082114702045264\n",
      "train loss:0.2742172122703627\n",
      "train loss:0.2227823391171016\n",
      "train loss:0.24993103949091863\n",
      "train loss:0.20756317392150853\n",
      "train loss:0.2103774467710107\n",
      "train loss:0.3091520191705854\n",
      "train loss:0.24478365857721676\n",
      "train loss:0.2058535538032844\n",
      "train loss:0.3253292790553495\n",
      "=== epoch:125, train acc:0.961, test acc:0.834 ===\n",
      "train loss:0.24707659857796582\n",
      "train loss:0.2186045263141306\n",
      "train loss:0.26505433752610225\n",
      "train loss:0.23491281122078536\n",
      "train loss:0.2532606380304768\n",
      "train loss:0.15244853442537007\n",
      "train loss:0.23058689980983288\n",
      "train loss:0.22038533606019306\n",
      "train loss:0.20599248020233482\n",
      "train loss:0.2937726888416955\n",
      "=== epoch:126, train acc:0.961, test acc:0.834 ===\n",
      "train loss:0.2403926925638329\n",
      "train loss:0.2245896950495258\n",
      "train loss:0.16873750622175865\n",
      "train loss:0.23370993947615748\n",
      "train loss:0.22362168496463103\n",
      "train loss:0.2052736494938953\n",
      "train loss:0.29328800133751215\n",
      "train loss:0.2461921673176231\n",
      "train loss:0.21070580596718125\n",
      "train loss:0.24193068847401783\n",
      "=== epoch:127, train acc:0.961, test acc:0.838 ===\n",
      "train loss:0.24100779174132203\n",
      "train loss:0.22175264195408922\n",
      "train loss:0.23408352349940426\n",
      "train loss:0.19485416923520174\n",
      "train loss:0.24911866371207014\n",
      "train loss:0.27559446157655837\n",
      "train loss:0.22339857755039635\n",
      "train loss:0.23156306123201273\n",
      "train loss:0.1657478507355794\n",
      "train loss:0.19715829134978882\n",
      "=== epoch:128, train acc:0.961, test acc:0.836 ===\n",
      "train loss:0.23501630121231673\n",
      "train loss:0.23173114500714895\n",
      "train loss:0.1923466416270098\n",
      "train loss:0.20604131707413217\n",
      "train loss:0.23917421605578063\n",
      "train loss:0.218792604883413\n",
      "train loss:0.17938004035074226\n",
      "train loss:0.2107168033245011\n",
      "train loss:0.22110733748714728\n",
      "train loss:0.2273353194847827\n",
      "=== epoch:129, train acc:0.961, test acc:0.836 ===\n",
      "train loss:0.19533307287439766\n",
      "train loss:0.27177953635512125\n",
      "train loss:0.21868629131702627\n",
      "train loss:0.2710074614670296\n",
      "train loss:0.21986024463167922\n",
      "train loss:0.2406262544830915\n",
      "train loss:0.2047650128442228\n",
      "train loss:0.21136912172307337\n",
      "train loss:0.1936208281848418\n",
      "train loss:0.20522964089758408\n",
      "=== epoch:130, train acc:0.961, test acc:0.838 ===\n",
      "train loss:0.17245814213018587\n",
      "train loss:0.21692508387938036\n",
      "train loss:0.1782306463215494\n",
      "train loss:0.16490345756655866\n",
      "train loss:0.16812394576977532\n",
      "train loss:0.27187398462969264\n",
      "train loss:0.2373922011595955\n",
      "train loss:0.23685362069836505\n",
      "train loss:0.20582704588530765\n",
      "train loss:0.24385526735927332\n",
      "=== epoch:131, train acc:0.962, test acc:0.84 ===\n",
      "train loss:0.30897356944890353\n",
      "train loss:0.2087767484851479\n",
      "train loss:0.22976940016880276\n",
      "train loss:0.20581159876087077\n",
      "train loss:0.22553845321194455\n",
      "train loss:0.20913357310294486\n",
      "train loss:0.19938447524896843\n",
      "train loss:0.2488181921956066\n",
      "train loss:0.19313015570624445\n",
      "train loss:0.1920934925314302\n",
      "=== epoch:132, train acc:0.962, test acc:0.84 ===\n",
      "train loss:0.2431619420700453\n",
      "train loss:0.2331595694010802\n",
      "train loss:0.20130184004138396\n",
      "train loss:0.21049348676313326\n",
      "train loss:0.22864222761965344\n",
      "train loss:0.2031037154377362\n",
      "train loss:0.27414062556274865\n",
      "train loss:0.17878304560193978\n",
      "train loss:0.2683232234687699\n",
      "train loss:0.24365680228153014\n",
      "=== epoch:133, train acc:0.961, test acc:0.838 ===\n",
      "train loss:0.22175439766774643\n",
      "train loss:0.22495034059134236\n",
      "train loss:0.1631163353817561\n",
      "train loss:0.19917830046922527\n",
      "train loss:0.26476161758498845\n",
      "train loss:0.17219601538227855\n",
      "train loss:0.2155847159861059\n",
      "train loss:0.26070264686221145\n",
      "train loss:0.2696005415113712\n",
      "train loss:0.1934368172739352\n",
      "=== epoch:134, train acc:0.961, test acc:0.838 ===\n",
      "train loss:0.21267340751943564\n",
      "train loss:0.18804367135686909\n",
      "train loss:0.21558381018550604\n",
      "train loss:0.1610884798447666\n",
      "train loss:0.18130346779346254\n",
      "train loss:0.17715336102331566\n",
      "train loss:0.15757054703505405\n",
      "train loss:0.2547245628511521\n",
      "train loss:0.209485534128458\n",
      "train loss:0.16851705752645135\n",
      "=== epoch:135, train acc:0.961, test acc:0.838 ===\n",
      "train loss:0.2081114487896004\n",
      "train loss:0.21673678084834527\n",
      "train loss:0.19469818488326912\n",
      "train loss:0.19459068584786934\n",
      "train loss:0.25057805643342085\n",
      "train loss:0.23159474581801112\n",
      "train loss:0.20735866320914745\n",
      "train loss:0.16560670768199384\n",
      "train loss:0.19174828044920453\n",
      "train loss:0.2000699329682347\n",
      "=== epoch:136, train acc:0.962, test acc:0.838 ===\n",
      "train loss:0.21793822469143564\n",
      "train loss:0.25254105614535544\n",
      "train loss:0.2445060237456303\n",
      "train loss:0.24083827502137928\n",
      "train loss:0.2517327781123978\n",
      "train loss:0.22994854451449112\n",
      "train loss:0.18630925950579535\n",
      "train loss:0.1835645348083521\n",
      "train loss:0.17451633077180134\n",
      "train loss:0.23332754637932612\n",
      "=== epoch:137, train acc:0.962, test acc:0.84 ===\n",
      "train loss:0.17664191812531535\n",
      "train loss:0.1688707431534705\n",
      "train loss:0.18795860685907254\n",
      "train loss:0.1978863846674696\n",
      "train loss:0.1769960918462424\n",
      "train loss:0.24527023241141485\n",
      "train loss:0.18370952249654832\n",
      "train loss:0.19562584974114522\n",
      "train loss:0.175846896541841\n",
      "train loss:0.20524888701351857\n",
      "=== epoch:138, train acc:0.963, test acc:0.84 ===\n",
      "train loss:0.1967475256697147\n",
      "train loss:0.16675439422392113\n",
      "train loss:0.19260597468552856\n",
      "train loss:0.1924358611884607\n",
      "train loss:0.20349232533916203\n",
      "train loss:0.2265279309476364\n",
      "train loss:0.18814530117010975\n",
      "train loss:0.17261419228757272\n",
      "train loss:0.25799016657535795\n",
      "train loss:0.22065583784952345\n",
      "=== epoch:139, train acc:0.962, test acc:0.84 ===\n",
      "train loss:0.2052224953608787\n",
      "train loss:0.2264438110301451\n",
      "train loss:0.14482666771101127\n",
      "train loss:0.2130676974429349\n",
      "train loss:0.1481681491777197\n",
      "train loss:0.19527611814713894\n",
      "train loss:0.17340503899781115\n",
      "train loss:0.23927583847301506\n",
      "train loss:0.17373509459752778\n",
      "train loss:0.20574906454041175\n",
      "=== epoch:140, train acc:0.962, test acc:0.84 ===\n",
      "train loss:0.18474006432934684\n",
      "train loss:0.2129983831565067\n",
      "train loss:0.19764352241509864\n",
      "train loss:0.18030774742802816\n",
      "train loss:0.14340847150429775\n",
      "train loss:0.20410856825030238\n",
      "train loss:0.16828673346860612\n",
      "train loss:0.14883985769160088\n",
      "train loss:0.18826256294872248\n",
      "train loss:0.18085793173098058\n",
      "=== epoch:141, train acc:0.962, test acc:0.84 ===\n",
      "train loss:0.19042497230510805\n",
      "train loss:0.21835643350407366\n",
      "train loss:0.22600124656295645\n",
      "train loss:0.1830840675502789\n",
      "train loss:0.2312450300655798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2043057169756476\n",
      "train loss:0.2322086259538862\n",
      "train loss:0.1858444458268337\n",
      "train loss:0.15749785598132943\n",
      "train loss:0.19606516274858826\n",
      "=== epoch:142, train acc:0.963, test acc:0.842 ===\n",
      "train loss:0.1794270307511876\n",
      "train loss:0.13509393455375782\n",
      "train loss:0.21615244715729764\n",
      "train loss:0.1826011986116645\n",
      "train loss:0.18406100651080798\n",
      "train loss:0.1808565049198717\n",
      "train loss:0.23600858445058\n",
      "train loss:0.21339188512959467\n",
      "train loss:0.15788137674972835\n",
      "train loss:0.19484432223078116\n",
      "=== epoch:143, train acc:0.964, test acc:0.842 ===\n",
      "train loss:0.19404573736965716\n",
      "train loss:0.16014687090051652\n",
      "train loss:0.18203389040038137\n",
      "train loss:0.2070729698705504\n",
      "train loss:0.20074426554989547\n",
      "train loss:0.1863279021268508\n",
      "train loss:0.1959447581207204\n",
      "train loss:0.1913609526928129\n",
      "train loss:0.21351220618573818\n",
      "train loss:0.19002868848945384\n",
      "=== epoch:144, train acc:0.964, test acc:0.842 ===\n",
      "train loss:0.20264105607922037\n",
      "train loss:0.21028537559750934\n",
      "train loss:0.2392154007264676\n",
      "train loss:0.16203264497166855\n",
      "train loss:0.1979674000279973\n",
      "train loss:0.15632262294887506\n",
      "train loss:0.2080052366054457\n",
      "train loss:0.1595920741663375\n",
      "train loss:0.16824444032967947\n",
      "train loss:0.17552653140080046\n",
      "=== epoch:145, train acc:0.964, test acc:0.842 ===\n",
      "train loss:0.18985712665285778\n",
      "train loss:0.18391411618660772\n",
      "train loss:0.22216049148653919\n",
      "train loss:0.19766500270009316\n",
      "train loss:0.180763570502843\n",
      "train loss:0.16736809730203978\n",
      "train loss:0.1383122297946741\n",
      "train loss:0.1891963383779788\n",
      "train loss:0.1785256438892192\n",
      "train loss:0.19181722817375943\n",
      "=== epoch:146, train acc:0.964, test acc:0.848 ===\n",
      "train loss:0.21989805541038177\n",
      "train loss:0.17396907867655945\n",
      "train loss:0.24313329091346092\n",
      "train loss:0.2356127075281778\n",
      "train loss:0.19364210762448336\n",
      "train loss:0.19146260765218867\n",
      "train loss:0.15107672511674297\n",
      "train loss:0.19215024928176094\n",
      "train loss:0.11850695015401079\n",
      "train loss:0.1414898588818114\n",
      "=== epoch:147, train acc:0.966, test acc:0.844 ===\n",
      "train loss:0.23629032877610814\n",
      "train loss:0.14954012333127717\n",
      "train loss:0.1889885925451827\n",
      "train loss:0.17853989444406454\n",
      "train loss:0.15794360966049217\n",
      "train loss:0.20554686477747164\n",
      "train loss:0.14540311361256905\n",
      "train loss:0.1932580635293229\n",
      "train loss:0.1336623909494887\n",
      "train loss:0.2225496695109775\n",
      "=== epoch:148, train acc:0.967, test acc:0.846 ===\n",
      "train loss:0.3244811035580101\n",
      "train loss:0.2063422671065441\n",
      "train loss:0.14337143982778533\n",
      "train loss:0.17058817273010088\n",
      "train loss:0.20530071803284336\n",
      "train loss:0.12127957700712261\n",
      "train loss:0.1457825363802178\n",
      "train loss:0.22821220240614742\n",
      "train loss:0.1475564856204387\n",
      "train loss:0.12627266354347993\n",
      "=== epoch:149, train acc:0.966, test acc:0.848 ===\n",
      "train loss:0.14964030091808844\n",
      "train loss:0.19733995499473717\n",
      "train loss:0.16596070159872187\n",
      "train loss:0.2055178915930799\n",
      "train loss:0.14105692372744183\n",
      "train loss:0.1727021598575602\n",
      "train loss:0.258781182433911\n",
      "train loss:0.18873756449009096\n",
      "train loss:0.17907129476529923\n",
      "train loss:0.17267345111190163\n",
      "=== epoch:150, train acc:0.966, test acc:0.848 ===\n",
      "train loss:0.19909744700367882\n",
      "train loss:0.1541886360333384\n",
      "train loss:0.1507550862084729\n",
      "train loss:0.19950343827366268\n",
      "train loss:0.166584693685286\n",
      "train loss:0.14399471551582027\n",
      "train loss:0.18560630375482248\n",
      "train loss:0.15876201825399222\n",
      "train loss:0.14887404505343008\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.848\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test accはテストデータの正答率（高ければ汎用性がある）'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "\n",
    "max_epochs = 150\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.1)\n",
    "\n",
    "\"\"\"あらかじめ実装した訓練関数に代入し学習させる\"\"\"\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,  \n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "\"\"\"パラメータの保存\"\"\"\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "\"\"\"正答率をグラフにプロット\"\"\"\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"test accはテストデータの正答率（高ければ汎用性がある）\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"フィルターを表示させましたが７８４ピクセルなので正直良くなっているのかわからないです\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
